{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/skullscript/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/skullscript/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/skullscript/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/skullscript/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA ANALYSIS\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import re\n",
    "\n",
    "movie_data = load_files(r\"./review_polarity/txt_sentoken\")\n",
    "x, y = movie_data.data, movie_data.target\n",
    "documents = []\n",
    "\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "print(x[0])\n",
    "#preprocessing the data getting rid of numbers special characters etc\n",
    "for sent in range(0, len(x)):\n",
    "    #removing special character\n",
    "    document = re.sub(r'\\W', ' ', str(x[sent]))\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    #print(document)\n",
    "    documents.append(document)\n",
    "    #print(\"------------------------\\n\")\n",
    "    \n",
    "print('DATA ANALYSIS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size of  500\n",
      "**********Bag of Words Feature***********\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.75389408 0.77570093 0.76875    0.73981191 0.74294671]\n",
      "All Accuracies Mean\n",
      "0.7562207272531957\n",
      "Standard Deviation\n",
      "0.01405221136521152\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       208\n",
      "           1       0.77      0.74      0.76       192\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.77      0.77      0.77       400\n",
      "weighted avg       0.77      0.77      0.77       400\n",
      "\n",
      "0.77\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.75389408 0.77570093 0.76875    0.73981191 0.74294671]\n",
      "All Accuracies Mean\n",
      "0.7562207272531957\n",
      "Standard Deviation\n",
      "0.01405221136521152\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77       208\n",
      "           1       0.75      0.78      0.76       192\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.77      0.77      0.77       400\n",
      "weighted avg       0.77      0.77      0.77       400\n",
      "\n",
      "accuracy score:  0.7675\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.75389408 0.77570093 0.76875    0.73981191 0.74294671]\n",
      "All Accuracies Mean\n",
      "0.7562207272531957\n",
      "Standard Deviation\n",
      "0.01405221136521152\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       208\n",
      "           1       0.76      0.76      0.76       192\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.76      0.76      0.76       400\n",
      "weighted avg       0.77      0.77      0.77       400\n",
      "\n",
      "accuracy score:  0.765\n",
      "\n",
      "\n",
      "************* TFIDF Feature **************\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.76012461 0.75077882 0.7625     0.75548589 0.76175549]\n",
      "All Accuracies Mean\n",
      "0.7581289612203244\n",
      "Standard Deviation\n",
      "0.004410231669074949\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.79       208\n",
      "           1       0.78      0.73      0.75       192\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.77      0.77      0.77       400\n",
      "weighted avg       0.77      0.77      0.77       400\n",
      "\n",
      "0.77\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.76012461 0.75077882 0.7625     0.75548589 0.76175549]\n",
      "All Accuracies Mean\n",
      "0.7581289612203244\n",
      "Standard Deviation\n",
      "0.004410231669074949\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77       208\n",
      "           1       0.74      0.79      0.77       192\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.77      0.77      0.77       400\n",
      "weighted avg       0.77      0.77      0.77       400\n",
      "\n",
      "accuracy score:  0.7675\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.76012461 0.75077882 0.7625     0.75548589 0.76175549]\n",
      "All Accuracies Mean\n",
      "0.7581289612203244\n",
      "Standard Deviation\n",
      "0.004410231669074949\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       208\n",
      "           1       0.76      0.76      0.76       192\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.76      0.76      0.76       400\n",
      "weighted avg       0.77      0.77      0.77       400\n",
      "\n",
      "accuracy score:  0.765\n",
      "Feature size of  1000\n",
      "**********Bag of Words Feature***********\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.76323988 0.80685358 0.78125    0.79310345 0.78683386]\n",
      "All Accuracies Mean\n",
      "0.7862561524038322\n",
      "Standard Deviation\n",
      "0.014326785405524746\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       208\n",
      "           1       0.80      0.84      0.82       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "0.82\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.76323988 0.80685358 0.78125    0.79310345 0.78683386]\n",
      "All Accuracies Mean\n",
      "0.7862561524038322\n",
      "Standard Deviation\n",
      "0.014326785405524746\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79       208\n",
      "           1       0.77      0.80      0.79       192\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.79      0.79      0.79       400\n",
      "weighted avg       0.79      0.79      0.79       400\n",
      "\n",
      "accuracy score:  0.79\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.76323988 0.80685358 0.78125    0.79310345 0.78683386]\n",
      "All Accuracies Mean\n",
      "0.7862561524038322\n",
      "Standard Deviation\n",
      "0.014326785405524746\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       208\n",
      "           1       0.82      0.80      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "accuracy score:  0.8175\n",
      "\n",
      "\n",
      "************* TFIDF Feature **************\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.72897196 0.75389408 0.771875   0.77429467 0.77742947]\n",
      "All Accuracies Mean\n",
      "0.7612930363089483\n",
      "Standard Deviation\n",
      "0.018115193615882128\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81       208\n",
      "           1       0.80      0.79      0.79       192\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.80      0.80      0.80       400\n",
      "weighted avg       0.80      0.81      0.80       400\n",
      "\n",
      "0.805\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.72897196 0.75389408 0.771875   0.77429467 0.77742947]\n",
      "All Accuracies Mean\n",
      "0.7612930363089483\n",
      "Standard Deviation\n",
      "0.018115193615882128\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       208\n",
      "           1       0.75      0.81      0.78       192\n",
      "\n",
      "    accuracy                           0.78       400\n",
      "   macro avg       0.78      0.78      0.78       400\n",
      "weighted avg       0.78      0.78      0.78       400\n",
      "\n",
      "accuracy score:  0.7775\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.72897196 0.75389408 0.771875   0.77429467 0.77742947]\n",
      "All Accuracies Mean\n",
      "0.7612930363089483\n",
      "Standard Deviation\n",
      "0.018115193615882128\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       208\n",
      "           1       0.82      0.80      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "accuracy score:  0.8175\n",
      "Feature size of  1500\n",
      "**********Bag of Words Feature***********\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.79750779 0.78816199 0.825      0.77115987 0.84012539]\n",
      "All Accuracies Mean\n",
      "0.8043910096778288\n",
      "Standard Deviation\n",
      "0.02496695650295221\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82       208\n",
      "           1       0.80      0.82      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "0.8175\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.79750779 0.78816199 0.825      0.77115987 0.84012539]\n",
      "All Accuracies Mean\n",
      "0.8043910096778288\n",
      "Standard Deviation\n",
      "0.02496695650295221\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       208\n",
      "           1       0.79      0.82      0.80       192\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.81      0.81      0.81       400\n",
      "\n",
      "accuracy score:  0.8075\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.79750779 0.78816199 0.825      0.77115987 0.84012539]\n",
      "All Accuracies Mean\n",
      "0.8043910096778288\n",
      "Standard Deviation\n",
      "0.02496695650295221\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83       208\n",
      "           1       0.81      0.80      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "accuracy score:  0.8175\n",
      "\n",
      "\n",
      "************* TFIDF Feature **************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.76323988 0.78504673 0.8        0.79937304 0.78996865]\n",
      "All Accuracies Mean\n",
      "0.7875256594302679\n",
      "Standard Deviation\n",
      "0.01340061366069831\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       208\n",
      "           1       0.82      0.78      0.80       192\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.81      0.81      0.81       400\n",
      "\n",
      "0.81\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.76323988 0.78504673 0.8        0.79937304 0.78996865]\n",
      "All Accuracies Mean\n",
      "0.7875256594302679\n",
      "Standard Deviation\n",
      "0.01340061366069831\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80       208\n",
      "           1       0.77      0.82      0.79       192\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.80      0.80      0.79       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n",
      "accuracy score:  0.795\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.76323988 0.78504673 0.8        0.79937304 0.78996865]\n",
      "All Accuracies Mean\n",
      "0.7875256594302679\n",
      "Standard Deviation\n",
      "0.01340061366069831\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83       208\n",
      "           1       0.81      0.80      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "accuracy score:  0.8175\n",
      "Feature size of  2000\n",
      "**********Bag of Words Feature***********\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.77258567 0.80996885 0.83125    0.77742947 0.81191223]\n",
      "All Accuracies Mean\n",
      "0.8006292419847851\n",
      "Standard Deviation\n",
      "0.022256847345454307\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.81       208\n",
      "           1       0.77      0.89      0.83       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.83      0.82      0.82       400\n",
      "weighted avg       0.83      0.82      0.82       400\n",
      "\n",
      "0.82\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.77258567 0.80996885 0.83125    0.77742947 0.81191223]\n",
      "All Accuracies Mean\n",
      "0.8006292419847851\n",
      "Standard Deviation\n",
      "0.022256847345454307\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.78      0.81       208\n",
      "           1       0.78      0.84      0.81       192\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.81      0.81      0.81       400\n",
      "\n",
      "accuracy score:  0.81\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.77258567 0.80996885 0.83125    0.77742947 0.81191223]\n",
      "All Accuracies Mean\n",
      "0.8006292419847851\n",
      "Standard Deviation\n",
      "0.022256847345454307\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       208\n",
      "           1       0.82      0.80      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "accuracy score:  0.82\n",
      "\n",
      "\n",
      "************* TFIDF Feature **************\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.7694704  0.73831776 0.79375    0.80877743 0.81818182]\n",
      "All Accuracies Mean\n",
      "0.7856994819285344\n",
      "Standard Deviation\n",
      "0.02884758384010001\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82       208\n",
      "           1       0.81      0.78      0.80       192\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.81      0.81      0.81       400\n",
      "\n",
      "0.8075\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.7694704  0.73831776 0.79375    0.80877743 0.81818182]\n",
      "All Accuracies Mean\n",
      "0.7856994819285344\n",
      "Standard Deviation\n",
      "0.02884758384010001\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80       208\n",
      "           1       0.77      0.83      0.80       192\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.80      0.80      0.80       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n",
      "accuracy score:  0.8025\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.7694704  0.73831776 0.79375    0.80877743 0.81818182]\n",
      "All Accuracies Mean\n",
      "0.7856994819285344\n",
      "Standard Deviation\n",
      "0.02884758384010001\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       208\n",
      "           1       0.82      0.80      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "accuracy score:  0.82\n",
      "Feature size of  2500\n",
      "**********Bag of Words Feature***********\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.84423676 0.78816199 0.834375   0.77429467 0.82758621]\n",
      "All Accuracies Mean\n",
      "0.8137309263274055\n",
      "Standard Deviation\n",
      "0.027414379553904668\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.85       208\n",
      "           1       0.82      0.87      0.84       192\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.85      0.85      0.84       400\n",
      "weighted avg       0.85      0.84      0.85       400\n",
      "\n",
      "0.845\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.84423676 0.78816199 0.834375   0.77429467 0.82758621]\n",
      "All Accuracies Mean\n",
      "0.8137309263274055\n",
      "Standard Deviation\n",
      "0.027414379553904668\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79       208\n",
      "           1       0.76      0.83      0.79       192\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.80      0.80      0.79       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n",
      "accuracy score:  0.795\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.84423676 0.78816199 0.834375   0.77429467 0.82758621]\n",
      "All Accuracies Mean\n",
      "0.8137309263274055\n",
      "Standard Deviation\n",
      "0.027414379553904668\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83       208\n",
      "           1       0.83      0.80      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.83      0.82      0.82       400\n",
      "weighted avg       0.83      0.82      0.82       400\n",
      "\n",
      "accuracy score:  0.825\n",
      "\n",
      "\n",
      "************* TFIDF Feature **************\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.77258567 0.76635514 0.76875    0.80877743 0.79623824]\n",
      "All Accuracies Mean\n",
      "0.7825412967900077\n",
      "Standard Deviation\n",
      "0.016895248306312102\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       208\n",
      "           1       0.81      0.72      0.77       192\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.79      0.79      0.79       400\n",
      "weighted avg       0.79      0.79      0.79       400\n",
      "\n",
      "0.7875\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.77258567 0.76635514 0.76875    0.80877743 0.79623824]\n",
      "All Accuracies Mean\n",
      "0.7825412967900077\n",
      "Standard Deviation\n",
      "0.016895248306312102\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78       208\n",
      "           1       0.75      0.83      0.79       192\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.79      0.79      0.78       400\n",
      "weighted avg       0.79      0.79      0.78       400\n",
      "\n",
      "accuracy score:  0.785\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.77258567 0.76635514 0.76875    0.80877743 0.79623824]\n",
      "All Accuracies Mean\n",
      "0.7825412967900077\n",
      "Standard Deviation\n",
      "0.016895248306312102\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83       208\n",
      "           1       0.83      0.80      0.81       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.83      0.82      0.82       400\n",
      "weighted avg       0.83      0.82      0.82       400\n",
      "\n",
      "accuracy score:  0.825\n",
      "Feature size of  3000\n",
      "**********Bag of Words Feature***********\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.78193146 0.79439252 0.790625   0.79937304 0.830721  ]\n",
      "All Accuracies Mean\n",
      "0.7994086062852176\n",
      "Standard Deviation\n",
      "0.016662717575316448\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       208\n",
      "           1       0.82      0.84      0.83       192\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.84      0.84      0.84       400\n",
      "\n",
      "0.8375\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.78193146 0.79439252 0.790625   0.79937304 0.830721  ]\n",
      "All Accuracies Mean\n",
      "0.7994086062852176\n",
      "Standard Deviation\n",
      "0.016662717575316448\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80       208\n",
      "           1       0.77      0.82      0.80       192\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.80      0.80      0.80       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n",
      "accuracy score:  0.7975\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.78193146 0.79439252 0.790625   0.79937304 0.830721  ]\n",
      "All Accuracies Mean\n",
      "0.7994086062852176\n",
      "Standard Deviation\n",
      "0.016662717575316448\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       208\n",
      "           1       0.85      0.81      0.83       192\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.84      0.84      0.84       400\n",
      "\n",
      "accuracy score:  0.8425\n",
      "\n",
      "\n",
      "************* TFIDF Feature **************\n",
      "\n",
      "\n",
      "RANDOM_FOREST_CLASSIFICATION\n",
      "All Accuracies\n",
      "[0.74766355 0.78816199 0.81875    0.78996865 0.78996865]\n",
      "All Accuracies Mean\n",
      "0.7869025698493151\n",
      "Standard Deviation\n",
      "0.022690734869119017\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       208\n",
      "           1       0.79      0.77      0.78       192\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.79      0.79      0.79       400\n",
      "weighted avg       0.79      0.79      0.79       400\n",
      "\n",
      "0.79\n",
      "\n",
      "\n",
      "NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.74766355 0.78816199 0.81875    0.78996865 0.78996865]\n",
      "All Accuracies Mean\n",
      "0.7869025698493151\n",
      "Standard Deviation\n",
      "0.022690734869119017\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.77       208\n",
      "           1       0.75      0.81      0.77       192\n",
      "\n",
      "    accuracy                           0.78       400\n",
      "   macro avg       0.78      0.78      0.77       400\n",
      "weighted avg       0.78      0.78      0.78       400\n",
      "\n",
      "accuracy score:  0.775\n",
      "\n",
      "\n",
      "BERNOULLI_NAIVE_BAYES\n",
      "All Accuracies\n",
      "[0.74766355 0.78816199 0.81875    0.78996865 0.78996865]\n",
      "All Accuracies Mean\n",
      "0.7869025698493151\n",
      "Standard Deviation\n",
      "0.022690734869119017\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       208\n",
      "           1       0.85      0.81      0.83       192\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.84      0.84      0.84       400\n",
      "\n",
      "accuracy score:  0.8425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def cross_validate(modelName, model, data, target):\n",
    "    print('\\n')\n",
    "    all_accuracies = cross_val_score(estimator=model, X= data, y=target, cv=5)\n",
    "    print(modelName)\n",
    "    print(\"All Accuracies\")\n",
    "    print(all_accuracies)\n",
    "    print(\"All Accuracies Mean\")\n",
    "    print(all_accuracies.mean())\n",
    "    print(\"Standard Deviation\")\n",
    "    print(all_accuracies.std())\n",
    "    print('\\n')\n",
    "\n",
    "def model_classification(x,y):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Random Forest Algorithm is used to train our model \n",
    "    # The fit method is used to train the algorithm passing training data and training target sets\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    #y_pred = classifier.predict(x_test)\n",
    "\n",
    "    # Evaluating the model using confusion matrix\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    #print(\"Random Forest Algorithm\")\n",
    "    #print(confusion_matrix(y_test, y_pred))\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    #print(accuracy_score(y_test,y_pred))\n",
    "    \n",
    "    cross_validate(\"RANDOM_FOREST_CLASSIFICATION\", classifier, x_train, y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    # Training and testing data set with Naive Bayes theorem\n",
    "\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    NB = MultinomialNB()\n",
    "    NB.fit(x_train, y_train)\n",
    "    cross_validate(\"NAIVE_BAYES\", classifier, x_train, y_train) \n",
    "    y_pred = NB.predict(x_test)\n",
    "    print('Confusion Matrix')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('accuracy score: ', str(accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "\n",
    "    from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "    BNB = BernoulliNB()\n",
    "    BNB.fit(x_train, y_train)\n",
    "    cross_validate(\"BERNOULLI_NAIVE_BAYES\", classifier, x_train, y_train) \n",
    "    y_pred = BNB.predict(x_test)\n",
    "    print('Confusion Matrix')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('accuracy score: ', str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "def test_diff_feature_sz():\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    sizes =[ 500, 1000, 1500, 2000, 2500, 3000 ]\n",
    "    for sz in sizes:\n",
    "        print('Feature size of ', str(sz) )\n",
    "        print('**********Bag of Words Feature***********')\n",
    "        vectorizer = CountVectorizer(max_features=sz, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "        x = vectorizer.fit_transform(documents).toarray()\n",
    "        model_classification(x,y)\n",
    "        print('\\n')\n",
    "        print('************* TFIDF Feature **************')\n",
    "        tfidfconverter = TfidfVectorizer(max_features=sz, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "        x = tfidfconverter.fit_transform(documents).toarray()\n",
    "        model_classification(x,y)\n",
    "\n",
    "test_diff_feature_sz()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "print(\"------------------------Baf of Words Feature-------------------------\")\n",
    "# converting text to number using the bag of words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# max feature 1500 min_numDoc = 5 contating feature, only include those word that occur max of 70% of all doc\n",
    "# word that occurs in every doc is not suitable for classification does not provide any unique information about doc\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "x = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "model_classification(x,y)\n",
    "print(\"------------------------TDIDF Feature---------------------------------\")\n",
    "# term frequency\n",
    "# bag of words drawback! It doesn't take avvount the fact that the word might also be having high frequency occurent\n",
    "# in other documents as well \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "x = tfidfconverter.fit_transform(documents).toarray()0.7562207272531957\n",
    "\n",
    "model_classification(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
